*Para o tensorflow rodar a operação, antes ele cria um mapa. Para rodar o mapa, voce precisa de uma sessao:
	Sess = tf.compat.v1.Session()#Criando um objeto da classe sessão. Repare qua apartir da versao 2.0.0 operacoes com constante nao entram no mapa
	
*Antes de rodar uma sessao, voce precisa inicializar todas as variaveis com o valores desejados:
		init = tf.compat.v1.global_variables_initializer()#Para inicializar as variaveis
	Em uma sessao, rode o 'init'


*a = tf.constant()#Cria uma constante.
*a = tf.zeros()#Cria uma variavel inicializada em zero com o shape designado e nome designado
*a = tf.ones()#Cria uma variavel inicializada em 1 com o shape designado e nome designado
*a = tf.random_normal()#Cria uma variavel inicializada em um valor random maior que 0 ate 1 com o shape designado e nome designado
*a = tf.Variable()#Cria uma variavel inicializada em zero com o shape designado e nome designado


*Em um modelo, uma variavel nunca consegue ser recuperada! Ela consegue ser compativel por nome e entao copiada em outra de mesmo nome (load):
	tf.compat.v1.get_variable()




*Para passar o valor de uma variavel durante o treinamento de um modelo, voce usa um placeholder
	tf.compat.v1.placeholder()
	
	Tive um problema: ao usar o placeholder surgiu um error: tf.placeholder() is not compatible with eager execution. A solução é dar disable no eager execution:
		tf.compat.v1.disable_eager_execution()#TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs


*Para criar variaveis em diferentes escopos:
	with tf.compat.v1.variable_scope('escopo1'):
	    var = tf.Variable(tf.compat.v1.random_normal([1],),name = 'var1')#Adicionando outra variavel de mesmo nome so que em escopo diferente
	    



*Operaçoes:

	x = tf.multiply(a,b)#Multiplica aritmeticamente dois elementos
	x = tf.add(a,b)#Soma dois elementos


*Multiprocessameto:
		from tensorflow.python.client import device_lib, print(device_lib.list_local_devices())#Printa todos os processadores possiveis para processamento
		with tf.device('/CPU:0'):
			<ACOES>
	Para configurar uma sessao do device:
		with tf.compat.v1.Session(config = tf.compat.v1.ConfigProto(log_device_placement=True)) as sess:
		

*Tensorboard:
	Criamos um objeto para escrever um arquivo:
		tensorboard_file =tf.compat.v1.summary.FileWriter( logdir, sess.graph)
	Dai vai surgir uma pasta no diretorio dito com um arquivo dentro!
	Abrir aquele diretorio e abrir terminal. No terminal chamar:
		tensorboard --logdir=<nome_da_pasta_ou_arquivo>
	Vai surgir um link... Nao fechar o server e colar aquilo no browser... Pronto!!



============================================================================================== Keras: ============================================================================


*model = keras.models.Sequential()#Criando objeto que cria um modelo sequencial

*model.add(keras.layers.Dropout(0.5))#Para colocar uma taxa de dropout na layer anterior

*model.summary()#Plota o summary sequencial do objeto modelo

*Antes de usar o keras na gpu, vc precisa configurar o proto.
	*a.astype('float32')#Mexendo com a precisao da variavel


*target_train = keras.utils.np_utils.to_categorical(target_train,10)#Hotenconding an target


*model.add(keras.layers.Dense(10, input_dim = 784, activation = 'softmax'))


*model.compile(optmizer = 'sgd', loss = 'categorical_crossentropy', metrics=['accuracy'])#Stochastic Gradient Decent, crossentropy, e acurácia - esse compile é as configuraçoes de treinamento.

score = model.evaluate(input_val, target_val, verbose=0)#Avaliando o modelo para o database de validaçao silenciosamenteo score retornando tera dois floats o primeiro é o erro e o segundo a accuracy


*history = model.fit(input_train, target_train, batch_size = batch, epochs = epochs , verbose = 1 , validation_data = (input_val,target_val), callbacks = [<o nome do objeto de uma funcao ai>]) #Treinando o modelo  com esses endereços... Verbose = 0 : silencioso, Verbose = 1 :barra de progresso com epochs em cada linha, Verbose = 2 : Somente os resultados de cada epoca em cada linha no final de cada epoca ele registra um callback e executa o objeto. O history é uma varaivel para armazenar o callback automatico do fit.	
	O objeto history tem uma funcao history que é um dicionario
		uma das chaves é 'accuracy' -  qual a acuracia por cada epoca
		Outra chave é 'val_accuracy'
		Outra chave é 'loss'
		Outra chave é 'val_loss'


Callback é usado para parar o treinamento da rede caso ele alcance o resultado planejado.
	*keras.callbacks.Callback()

É possivel recordar todos os valores ja treinados em
	*keras.callbacks.History()
	*https://keras.io/callbacks/       para ensinar como usar

Checkpoint: sua rede é salva periodica por uma funcao callback. Ela consegue ser loadada e continuar treinando mais a frente
	*keras.callbacks.ModelCheckpoint(filepath,monitor = 'val_loss', verbose = 0, save_best_only = False, save_weights_only=False, mode = 'auto', period = 1)
		usar em: *model.fit(callbacks = [checkpoint])


Para salvar os pesos ou modelo:
	*model.save_weights(filepath,overwrite=True)# A extensao deve ser h5
	*model.save(filepath,overwrite=True)# A extensao deve ser h5
	*model = keras.models.load_model('/home/salomao/Desktop/saved.h5')#para loadar uma rede


Para usar o modelo loadado ou o treinado so usar
	*pred = model.predict(x)#tenha em mente que x tem que ter uma dimensao a mais, do batch
	Em casos de onehotencoded, voce pode usar a funcao a baixo para descobrir qual a coluna é a sobresalente.		
		np.argmax(pred[0])







